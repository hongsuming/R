# SVM (Support Vector Machine) : 서로 다른 집단에 속하는 데이터 간에 
# 간격이 최대가 되는 초평면을 기준으로 데이터를 분류하는 알고리즘
# 초평면 마진을 최대화하는데 결정적 영향을 주는 데이터들을 support vector라고 함 
# Kernal trick을 이용해서 비선형 데이터인 경우에도 차원을 늘려 분류가 가능.

set.seed(1)
x <- matrix(rnorm(20 * 2), ncol=2) # 정규 분포를 따른 난수 발생

y <- c(rep(-1, 10), rep(1, 10))
x #              [,1]        [,2]
  # [1,] -0.62645381  0.91897737
  # [2,]  0.18364332  0.78213630
  # [3,] -0.83562861  0.07456498
  # [4,]  1.59528080 -1.98935170
  # [5,]  0.32950777  0.61982575
  # [6,] -0.82046838 -0.05612874
  # [7,]  0.48742905 -0.15579551
  # [8,]  0.73832471 -1.47075238
  # [9,]  0.57578135 -0.47815006
  # [10,] -0.30538839  0.41794156
  # [11,]  1.51178117  1.35867955
  # [12,]  0.38984324 -0.10278773
  # [13,] -0.62124058  0.38767161
  # [14,] -2.21469989 -0.05380504
  # [15,]  1.12493092 -1.37705956
  # [16,] -0.04493361 -0.41499456
  # [17,] -0.01619026 -0.39428995
  # [18,]  0.94383621 -0.05931340
  # [19,]  0.82122120  1.10002537
  # [20,]  0.59390132  0.76317575
y # [1] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1

x[y == 1, ] <- x[y == 1, ] + 1 # x 값의 11번째부터 1을 얹어줌 (10번째 숫자 이후로는 1씩 커짐)
x #             [,1]        [,2]
  # [1,] -0.6264538  0.91897737
  # [2,]  0.1836433  0.78213630
  # [3,] -0.8356286  0.07456498
  # [4,]  1.5952808 -1.98935170
  # [5,]  0.3295078  0.61982575
  # [6,] -0.8204684 -0.05612874
  # [7,]  0.4874291 -0.15579551
  # [8,]  0.7383247 -1.47075238
  # [9,]  0.5757814 -0.47815006
  # [10,] -0.3053884  0.41794156
  # [11,]  2.5117812  2.35867955
  # [12,]  1.3898432  0.89721227
  # [13,]  0.3787594  1.38767161
  # [14,] -1.2146999  0.94619496
  # [15,]  2.1249309 -0.37705956
  # [16,]  0.9550664  0.58500544
  # [17,]  0.9838097  0.60571005
  # [18,]  1.9438362  0.94068660
  # [19,]  1.8212212  2.10002537
  # [20,]  1.5939013  1.76317575

plot(x, col=(2-y))

# SVM 모델
# install.packages("e1071")
library(e1071)

df <- data.frame(x=x, y=as.factor(y))
df #           x.1         x.2  y
   # 1  -0.6264538  0.91897737 -1
   # 2   0.1836433  0.78213630 -1
   # 3  -0.8356286  0.07456498 -1
   # 4   1.5952808 -1.98935170 -1
   # 5   0.3295078  0.61982575 -1
   # 6  -0.8204684 -0.05612874 -1
   # 7   0.4874291 -0.15579551 -1
   # 8   0.7383247 -1.47075238 -1
   # 9   0.5757814 -0.47815006 -1
   # 10 -0.3053884  0.41794156 -1
   # 11  2.5117812  2.35867955  1
   # 12  1.3898432  0.89721227  1
   # 13  0.3787594  1.38767161  1
   # 14 -1.2146999  0.94619496  1
   # 15  2.1249309 -0.37705956  1
   # 16  0.9550664  0.58500544  1
   # 17  0.9838097  0.60571005  1
   # 18  1.9438362  0.94068660  1
   # 19  1.8212212  2.10002537  1
   # 20  1.5939013  1.76317575  1
svmfit <- svm(formula=y ~ ., data=df, kernel='linear', cost=10, scale=F) # scale : 정규화화 
svmfit # Parameters:
       # SVM-Type:  C-classification 
       # SVM-Kernel:  linear 
       # cost:  10 
       # 
       # Number of Support Vectors:  7 // 7개가 참여함

plot(svmfit, df)

attributes(svmfit) # 모든 속성 목록

svmfit$index # 1  2  5  7 14 16 17 // index 속성을 이용해서 어떤 게 support vector인지 알려줌 
df[c(1, 2, 5, 7, 14, 16, 17), ] #           x.1        x.2  y
                                # 1  -0.6264538  0.9189774 -1
                                # 2   0.1836433  0.7821363 -1
                                # 5   0.3295078  0.6198257 -1
                                # 7   0.4874291 -0.1557955 -1
                                # 14 -1.2146999  0.9461950  1
                                # 16  0.9550664  0.5850054  1
                                # 17  0.9838097  0.6057100  1
summary(svmfit) # Number of Support Vectors:  7

svmfit <- svm(formula=y ~ ., data=df, kernel='linear', cost=0.1, scale=F) 
plot(svmfit, df)
summary(svmfit) # Number of Support Vectors:  16 
